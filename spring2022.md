---
layout: page
title: Spring 2022 
subtitle: 
---

<style>
    @media only screen and (min-width: 865px) {
        .row {
            margin-right: -100px;
            margin-left: -100px;
        }
    }
</style>

15 April, 2022

**Title**: The Atiyah-Jänich Theorem

**Speaker**: Mr. Satwata Hans, 5th Year BS-MS, IISER Kolkata

<p style="text-align:justify">
    <strong>Abstract</strong> : The space of Fredholm operators over a separable Hilbert space $\mathcal{H}$ has been an important area of study in Functional Analysis. It has gained more relevance over time due to the significant works of Sir Michael Atiyah and multiple other mathematicians, which have focused on various topological and geometric connections to the Fredholm operators. In this talk, we will see a famous theorem called the Atiyah-Jänich Theorem, which states that the space of Fredholm operators gives an alternate characterization of the $K$-Theory of any compact Hausdorff space.
</p>

---

8 April, 2022

**Title** : The Sensitivity Conjecture and Fourier Analysis of Boolean functions

**Speaker** : Mr. Debmalya Bandyopadhyay, 5th Year BS-MS, IISER Kolkata

<p style="text-align:justify">
    <strong>Abstract</strong> : One of the most celebrated unsolved problems in theoretical computer science over the last three decades has been the sensitivity conjecture. After about 30 years since it was conjectured, in 2019, Hao Huang gave a 3-page proof of the conjecture using spectral techniques. The talk will introduce the conjecture and go on to explain Huang's simple proof. It will then touch upon some basics of Fourier Analysis of Boolean functions to introduce a corollary of another unsolved problem: The Fourier Entropy Influence (FEI) conjecture. Towards the end, I would be talking about the classification of flat, homogeneous, and Boolean polynomials, that I have been working on for the last few months, in connection to the FEI conjecture. 
</p>

<p>
    <a href="/assets/slides/GSS_Talk_Debmalya.pdf" target = "_blank">Slides</a>
</p>


---

1 April, 2022

**Title** : Dimensionality Reduction for Multivariate and Functional Data

**Speaker** : Mr. Avishek Chatterjee, PhD Student, IISER Kolkata

<p style="text-align:justify">
    <strong>Abstract</strong> : Dimensionality reduction as a preprocessing step to machine learning is effective in removing irrelevant and redundant information, increasing learning accuracy, and improving result comprehensibility. The complexity of any classifier depends on the number of inputs. This determines both time and space complexity and the necessary number of training instances to train such a classifier. In this talk, we discuss Principal Component Analysis (PCA) and Functional Principal Component Analysis (FPCA), the most popular and widely used dimensionality reduction techniques for multivariate and functional data respectively. Further, we talk about a local FPCA based classifier, which is especially suited for hard classification framework. 
</p>

<p>
    <a href="" target = "_blank">Slides</a>
</p>

---


